{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac4e09-3327-430a-839e-d063959fef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fd131-8e50-47a9-bece-540f981c5c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"/content/drive/My Drive/Colab Notebooks/CoE202/Content-based Filtering\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3e1d6-3545-4f86-bc68-e627913dc546",
   "metadata": {},
   "source": [
    "# Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a652ff-9c4c-4993-868f-70584586a7a3",
   "metadata": {},
   "source": [
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=18-785SB1H_aWDukQ8fzvi6nqoH2d2ZXK\" width=\"40%\" height=\"40%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "  <figcaption>[Infographic by GO-Globe Web Design Company]</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Why Recommendation?\n",
    "\n",
    "- To identify things that we might like\n",
    "- To help people discover new content\n",
    "- To discover which things go together\n",
    "- To personalize user experiences in response to user feedback\n",
    "\n",
    "\"The most powerful AI space for the next a couple of decades is recommendation systems.  \n",
    "They are going to have the biggest impact on our society because they **affect the information we see, how we learn, what we think, how we communicate.**  \n",
    "These algorithms are controlling us ...\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa7f98-c422-4d97-9d74-c7b28b2f79a4",
   "metadata": {},
   "source": [
    "## Content-based vs. Collaborative Filtering\n",
    "\n",
    "- **Collaborative Filtering (CF)** (<span style=\"color:red\">Only Ratings</span>)\n",
    "    - Memory-based approach (Neighborhood-based CF)\n",
    "    - Model-based approach\n",
    "    \n",
    "- **Side information-based Recommendation**\n",
    "    - Content-based approach (<span style=\"color:red\">Only Contents</span>)\n",
    "    - Content-based CF (<span style=\"color:red\">Rating + Contents</span>)\n",
    "        - Text, Image, Social network ... \n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=197ni5M07eTb6hliZH3niu0YyP_4mfF6F\" width=\"40%\" height=\"40%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "  <figcaption>[Picture from https://www.themarketingtechnologist.co/]</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b1a00a-6da0-4af4-87f2-b2ff6795af55",
   "metadata": {},
   "source": [
    "## Content-based recommendation\n",
    "**Goal** : Recommend items **similar** to those the user liked  \n",
    "Content-based filtering uses item features to recommend other items similar to what the users likes, based on their previous actions or explicit feedback.\n",
    "\n",
    "#### Example\n",
    "- <span style=\"color:blue\">Movie recommendations</span> : Recommend movies with same actor, director, genre\n",
    "- <span style=\"color:blue\">Websites, blogs, news</span> : Recommend other sites with \"similar\" content\n",
    "\n",
    "\n",
    "#### When is it useful?\n",
    "<span style=\"color:red\">**Useful when ratings of other users are not available**</span>  \n",
    "\n",
    "Example: John has rated the movie “Terminator” highly, but we do not have access to the ratings of other users.\n",
    "- Therefore, we cannot use Collaborative Filtering (CF).\n",
    "- The item description of “Terminator” contains similar genre keywords as other science fiction movies, such as “Matrix”. \n",
    "- Therefore, “Matrix” can be recommended to John.\n",
    "\n",
    "<span style=\"color:red\">**Then, how can we find similar items?**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc1c22-f42c-4282-abeb-584f9904886d",
   "metadata": {},
   "source": [
    "#### Bag-of-Words (BoW)\n",
    "\n",
    "**Bag-of-Words (BoW)** simplifies representation of a document as the bag of its words, disregarding grammar and even word order but keeping multiplicity.  \n",
    "The bag-of-words model is commonly used in methods of **document classification** where the occurence of each word is used as feature for training a classifier.\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=19JlenTX2aZVjRaR4LFwUBc41Vc7KWjAN\" width=\"40%\" height=\"40%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Each document is represented by a **binary vector** of existing word.\n",
    "\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=19HEdheo7WX71eLuYYiEOKABeQ-v4o7nQ\" width=\"50%\" height=\"50%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "Let's see how to use Bag-of-Words in Natural Language Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b488faba-e674-4efc-931f-d4eaf46a7872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/namkyeong/anaconda3/envs/HDGI/lib/python3.7/site-packages (3.6.3)\n",
      "Requirement already satisfied: joblib in /home/namkyeong/anaconda3/envs/HDGI/lib/python3.7/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in /home/namkyeong/anaconda3/envs/HDGI/lib/python3.7/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: click in /home/namkyeong/anaconda3/envs/HDGI/lib/python3.7/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /home/namkyeong/anaconda3/envs/HDGI/lib/python3.7/site-packages (from nltk) (4.49.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/namkyeong/anaconda3/envs/HDGI/lib/python3.7/site-packages (from click->nltk) (4.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/namkyeong/anaconda3/envs/HDGI/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/namkyeong/anaconda3/envs/HDGI/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/namkyeong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/namkyeong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/namkyeong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0242758-b401-4b17-ae05-e889a3812aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The first time you see The Second Renaissance it may look boring.\",\n",
    "          \"Look at it at least twice and definitely watch part 2.\", \n",
    "          \"It will change your view of the matrix\",\n",
    "          \"Are the human people the ones who started the war?\",\n",
    "          \"Is AI a bad thing?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eafb6590-46f6-40d2-88f5-54891f514dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\") # to remove the meaningless words\n",
    "lemmatizer = WordNetLemmatizer() # to find the root of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "092660dd-1a53-4efc-9e8b-5a929d0b2b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "   \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "   \n",
    "    # lemmatize and remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "   \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff1e2b4-bd81-427f-a6d2-f2e0fd716f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'first',\n",
       " 'time',\n",
       " 'you',\n",
       " 'see',\n",
       " 'The',\n",
       " 'Second',\n",
       " 'Renaissance',\n",
       " 'it',\n",
       " 'may',\n",
       " 'look',\n",
       " 'boring',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6bf7c89-806a-4605-8bad-ffa80f462be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first', 'time', 'see', 'second', 'renaissance', 'may', 'look', 'boring']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8bffe2-34b2-45c2-817e-156776e9ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize count vectorizer object\n",
    "vect = CountVectorizer(tokenizer = tokenize)\n",
    "\n",
    "# get counts of each token(word) in text data\n",
    "X = vect.fit_transform(corpus)\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d88ba2-72af-48ed-9760-a70a7a4a7049",
   "metadata": {},
   "source": [
    "But this model ignores relative importance between the words. For example, words like 'may' would be frequently used in many documents and may lead to distortion of similarity.  \n",
    "To overcome the problem, TF-IDF consider the relative importance of a term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda94f0d-80d5-453f-a8d4-d38a367d8787",
   "metadata": {},
   "source": [
    "#### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "**Term Frequency (TF)** : The number of occurences of a term  \n",
    "**Document Frequency (DF)** : The number of documents in the collection that the term occurs in.  \n",
    "We assume that rare terms are more informative than frequent terms.  \n",
    "> **Inverse Document Frequency (IDF)** : A measure of the informativeness of the term.  \n",
    "${idf}_{t} = \\log\\frac{N}{df_t}$\n",
    "\n",
    "\n",
    "**TF-IDF** is used to evaluate how important a word is to corpus of documents.   \n",
    "$tf-idf_{t} = (1 + \\log{{tf}_{t,d}})\\log{\\frac{N}{df_t}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84edf34d-6921-42e4-bd6d-812eea607c92",
   "metadata": {},
   "source": [
    "## Content-based movie recommendation system\n",
    "We are going to build **content-based movie recommendation system**.  \n",
    "The datasets contain metadata for all 45,000 movies listed in Full MovieLens Dataset. The datasets consists of movies released on or before July 2017.  \n",
    "- movie_metadat.csv\n",
    "    - The main Movies Metadata file. Contains information on 45,000 movies featured in the Full MovieLens dataset. Features include posters, backdrops, budget, revenue, release dates, languages, production countries and companies\n",
    "    \n",
    "- keywords.csv\n",
    "    - The file containing the movie plot keywords for our MovieLens movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1a0b3-e6b7-4851-8fc8-d97810dd1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21773865-b4e1-4db9-8c19-7c98f5f0e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie metadata\n",
    "movie_data = pd.read_csv('./movies_metadata.csv')\n",
    "\n",
    "print(movie_data.shape)\n",
    "print(movie_data.columns)\n",
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c66c2c1-f858-4866-83d3-86e9187be184",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data =  movie_data.loc[movie_data['original_language'] == 'en', :]\n",
    "movie_data = movie_data[['id', 'title', 'original_language', 'genres']]\n",
    "\n",
    "print(movie_data.shape)\n",
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09790a6-2f3c-4336-8105-1dd71aca1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie keywords data\n",
    "movie_keyword = pd.read_csv('./keywords.csv')\n",
    "\n",
    "print(movie_keyword.shape)\n",
    "movie_keyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6d4f4-3678-4935-8a0a-ff27d5a9340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge metadata and keywords data\n",
    "movie_data.id = movie_data.id.astype(int)\n",
    "movie_keyword.id = movie_keyword.id.astype(int)\n",
    "movie_data = pd.merge(movie_data, movie_keyword, on='id')\n",
    "\n",
    "print(movie_data.shape)\n",
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9fc6be-5bbb-471b-8d9a-bd0b85669fce",
   "metadata": {},
   "source": [
    "Movie genres and keywords are expressed in list and dictionary type. But the problem is that, it is actually consisted with string type. So we are going to change the string to dictionary and list with **ast.literal_eval** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e947453-185f-4a9e-80b8-bb5188bbeef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data.genres[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10100667-4ee2-4759-b779-676cd3bfdf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data['genres'] = movie_data['genres'].apply(literal_eval)\n",
    "movie_data['genres'] = movie_data['genres'].apply(lambda x : [d['name'] for d in x]).apply(lambda x : \" \".join(x))\n",
    "movie_data['genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d079c-e2bd-4803-a325-9c299fdc2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data['keywords'] = movie_data['keywords'].apply(literal_eval)\n",
    "movie_data['keywords'] = movie_data['keywords'].apply(lambda x : [d['name'] for d in x]).apply(lambda x : \" \".join(x))\n",
    "\n",
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba13a1-2b72-4638-861f-7f05a190bbb4",
   "metadata": {},
   "source": [
    "#### Create TF-IDF movie matrix\n",
    "Now, we are going to create TF-IDF matrix of the movie datasets.  \n",
    "We can simply adopt TF-IDF with only single line of code thanks to scikit-learn library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83a228-cc11-4109-b179-d2b608782f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vector.fit_transform(movie_data['genres'] + \" \" + movie_data['keywords']).toarray()\n",
    "tfidf_matrix_feature = tfidf_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452cdf31-2987-4acb-b709-b52898c5e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1ea07-5606-457b-aeb5-3a99dc25fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = pd.DataFrame(tfidf_matrix, columns=tfidf_matrix_feature, index = movie_data.title)\n",
    "tfidf_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d57cf0a-a502-4da5-8353-b5e369b5a484",
   "metadata": {},
   "source": [
    "#### Compute Cosine Similarity\n",
    "Now, we are going to compute cosine similarity based on the above TF-IDF matrix.  \n",
    "As you learned in the **Neighborhood-based Collaborative Filtering** lecture, cosine similarity can measure the similarity between two vectors.  \n",
    "A row of the above TF-IDF matrix can be seen as **a vector indicating the features of the movie**, and we use the vector to measure the similarity between movies.\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=18totRSZ2rFoZ7zRMHbjb6wW4Pv90dswO\" width=\"40%\" height=\"40%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "Cosine similarity is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction.\n",
    "\n",
    "We can also adopt cosine similarity with a single code, again thanks to scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7ace5-b77e-4d4a-9510-5d5a22318fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ea2d7-f1d3-4d53-8e2d-96e8a446c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_df = pd.DataFrame(cosine_sim, index = movie_data.title, columns = movie_data.title)\n",
    "print(cosine_sim_df.shape)\n",
    "cosine_sim_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15af428c-fb4a-4051-bb5c-b2e232635c27",
   "metadata": {},
   "source": [
    "#### Build Recommendation Function\n",
    "Now, we are going to build simple recommendation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dea064-027e-4891-b40f-9e9b729c7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(target_title, matrix, items, k=10):\n",
    "    \n",
    "    recom_idx = matrix.loc[:, target_title].values.reshape(1, -1).argsort()[:, ::-1].flatten()[1:k+1]\n",
    "    recom_title = items.iloc[recom_idx, :].title.values\n",
    "    recom_genre = items.iloc[recom_idx, :].genres.values\n",
    "    target_title_list = np.full(len(range(k)), target_title)\n",
    "    target_genre_list = np.full(len(range(k)), items[items.title == target_title].genres.values)\n",
    "    d = {\n",
    "        'target_title':target_title_list,\n",
    "        'target_genre':target_genre_list,\n",
    "        'recom_title' : recom_title,\n",
    "        'recom_genre' : recom_genre\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98983388-4fc2-4563-afec-e1ffd5d0da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('Jumanji', cosine_sim_df, movie_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075cf9b-912a-46be-ae93-556bf37fc480",
   "metadata": {},
   "source": [
    "Let's check out whether *Jumanji* and *The Games Maker* are really similar contents!\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=18VXzn0Ky306mbero4yIZ0AjvKGIr776p\" width=\"40%\" height=\"40%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417491e8-29c4-44c9-9b53-50da303b4d04",
   "metadata": {},
   "source": [
    "#### Pros and Cons of Content-based Filtering\n",
    "- **Pros**\n",
    "    - No need for other users data : No cold-start or sparsity\n",
    "    - Able to recommend to users with unique tastes\n",
    "    - Able to recommend new and unpopular items\n",
    "    - Able to provide explanations\n",
    "        - Can provide explanations of recommended items by listing content-features that caused an item to be recommended\n",
    "        \n",
    "- **Cons**\n",
    "    - Requires content that can be encoded as meaningful features (difficult in some domains/catalogs)\n",
    "    - Difficult to implement serendipity (Obvious recommendations)\n",
    "    - Easy to overfit (e.g. for a user with few data points)\n",
    "    - Effective for providing recommendations for new items, but not for new users\n",
    "    \n",
    "<span style=\"color:red\">**Pure content-based systems are rarely found in commercial environments**</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
