{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d7c676-4b78-4d5d-8802-6bad7dc7e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d79fb-64a9-4688-a3cb-7dca2c26f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"/content/drive/My Drive/Colab Notebooks/CoE202/Collaborative Filtering\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcff133-832d-4654-8a37-7fe6877c2d29",
   "metadata": {},
   "source": [
    "## Collaborative Filtering (CF)\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=185fQI_jd3DewJRSKO7TEIZH6Sjr15UGc\" width=\"50%\" height=\"50%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "- The most prominent approach to generate recommendations\n",
    "    - Used by large, commercial e-commerce sites\n",
    "    - Well-understood, various algorithms and variations exist\n",
    "    - Applicable in many domains\n",
    "- Use the **wisdom of the crowd** to recommend items\n",
    "- Basic assumption and idea\n",
    "    - Users give ratings to items (implicitly or explicitly)\n",
    "    - <span style=\"color:red\">**Customers who had similar tastes in the past will have similar tastes in the future**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2befc68a-000e-4180-883a-769802c4012f",
   "metadata": {},
   "source": [
    "### Neighborhood-based CF\n",
    "- Main idea\n",
    "    - Similar users display similar patterns of rating behavior (<span style=\"color:red\">**User-based CF**</span>)\n",
    "    - Similar items receive similar ratings (<span style=\"color:red\">**Item-based CF**</span>)\n",
    "- How do we define similarity between users and items?\n",
    "    - We define similarity between users in terms of items they purchased!\n",
    "    - We define similarity between items in terms of users who purchased them!\n",
    "    - We learned variety of similarity measures in the lecture (e.g., Euclidean distance, Jaccard similarity, Cosine similarity, Pearson correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ce4fa-bc1f-4a22-9546-5217ff88bb2e",
   "metadata": {},
   "source": [
    "### Model-based CF\n",
    "#### Matrix Factorization\n",
    "As we learned in the lecture, we can factorize **ratings matrix** into **user matrix** and **item matrix**.\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1880BHOvpFW66QjjjnnN-exW_HOyX9EkU\" width=\"50%\" height=\"50%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "Ratings can be interpreted as **dot product** of user latent and item latent.  \n",
    "So we can obtain user/item matrix by decomposing ratings matrix,  \n",
    "and predict unobserved ratings with obtained user/item matrix.\n",
    "\n",
    "Okay then, how can we decompose the ratings matrix into user and item latent matrix?\n",
    "\n",
    "**Singular Value Decomposition (SVD)** is a famous linear algebra technique for matrix factorization.  \n",
    "And it is often used for **dimensionality reduction**.  \n",
    "\n",
    "If you are not familiar with SVD, please refer to basic linear algebra class or chapter 4 in below linked book. (https://mml-book.github.io/book/mml-book.pdf)\n",
    "\n",
    "\n",
    "However, applying SVD to recommender system have the following issues.\n",
    "- Predicted values are often negative.\n",
    "- Zero replacement decreases prediction quality.\n",
    "    - <span style=\"color:red\">The meaning of \"zero\" is different from that of \"unknown\"</span>.\n",
    "    - We should be careful whether we want to set missing values to zero.\n",
    "\n",
    "\n",
    "So, we can model directly leveraging **only observed ratings**, while **avoiding overfitting** through an adequate regularized model, such as :  \n",
    "$min \\frac{1}{2} \\sum_{(u, i)\\in R}{(r_{ui} -\\mu -b_{u}^{user} -b_{i}^{item} -p_{u}q_{i}^{T})^{2}} + \\lambda(|p_{u}|^{2} + |q_{i}|^{2} + {b_{u}^{user}}^{2} + {b_{i}^{item}}^{2})$  \n",
    "where $p_{u}$ and $q_{i}$ are the latent factor of user $u$ and item $i$, respectively.  \n",
    "$b_{u}$ and $b_{i}$ are the bias term of user $u$ and item $i$, respectively.  \n",
    "$\\mu$ is the mean of the observed ratings matrix.  \n",
    "\n",
    "\n",
    "A simple **gradient descent** technique can be applied to solve the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000571ba-ff00-43f2-a05e-3f9cdc1a2162",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "In optimization problem, we have two approaches to find solution of the problem.  \n",
    "First, we can simply solve the problem in **closed form**.  \n",
    "However, in the case of complex function, it may not be feasible to solve in the closed form.  \n",
    "To address the issue, we can iteratively search the solution space improving the target value. We call such a method as **improving search**.\n",
    "\n",
    "Gradient descent is a first-order iterative optimization algorithm for finding local minimum of a differentiable function.  \n",
    "The idea is to take repeated steps in the **opposite direction of the gradient** of the function at the current point, because this is the direction of steepest descent.\n",
    "\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=18o3tBBj9roZsHkLXdatayiGc3faF0zrM\" width=\"50%\" height=\"50%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "If gradient > 0 , then increasing the weight will increase the cost function.  \n",
    "If gradient < 0 , then increasing the weight will decrease the cost function.\n",
    "\n",
    "So we update the weight (parameters) with the partial derivative of the cost.  \n",
    "$w \\leftarrow w - \\alpha \\frac{\\partial Cost}{\\partial w}$\n",
    "\n",
    "\n",
    "\n",
    "Gradient descent is very general algorithm which is very **effective** and **scalable**.  \n",
    "However, we should use gradient descent taking the below in regard.\n",
    "- **Local minima**\n",
    "    - Sensitive to the starting point\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=18loEZYzhQAZj0A6192fwIGZ0M2rHeXUO\" width=\"40%\" height=\"40%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "- **Learning rate**\n",
    "    - Too large? Too small?\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=18ZvvZVl3oTAo9mzILltbE-S-3Cq_a3Yt\" width=\"40%\" height=\"40%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a7748e-86bb-43f1-8803-96c12684d969",
   "metadata": {},
   "source": [
    "#### Type of Gradient Descent\n",
    "- **Batch Gradient Descent**\n",
    "    - Let's say there are a total of 'm' observations in a data set and we use all these observations to calculate the cost function J, then this is known as **Batch Gradient Descent**.\n",
    "    - So we take the entire training set, perform forward propagation and calculate the cost function.\n",
    "    - And then we update the parameters using the rate of change of this cost function with respect to the parameters.\n",
    "\n",
    "\n",
    "- **Stochastic Gradient Descent**\n",
    "    - If you use a **single observation** to calculate the cost function it is known as Stochastic Gradient Descent, commonly abbreviated as SGD.\n",
    "    - We pass a single observation at a time, calculate the cost and update the parameters.\n",
    "\n",
    "\n",
    "- **Mini-batch Gradient Descent**\n",
    "    - Another type of Gradient Descent is the Mini-batch Gradient Descent. \n",
    "    - It takes a **subset of the entire dataset** to calculate the cost function. \n",
    "    - So if there are ‘m’ observations then the number of observations in each subset or mini-batches will be more than 1 and less than ‘m’.\n",
    "\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1KWGGBGuqjW-idlOEHGJoZNO4x-4_J9DT\" width=\"40%\" height=\"40%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "Let's implement Matrix Factorization using Stochastic Gradient Descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d151f79-1e82-463e-96c3-da1abd55970d",
   "metadata": {},
   "source": [
    "#### Matrix Factorization\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=186dDOMvTxDXGrVqW1kke74zMQT3DvE9e\" width=\"70%\" height=\"70%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "We can minimize the below cost function with gradient descent.  \n",
    "$min \\frac{1}{2} \\sum_{(u, i)\\in R}{(r_{ui} -\\mu -b_{u}^{user} -b_{i}^{item} -p_{u}q_{i}^{T})^{2}} + \\lambda(|p_{u}|^{2} + |q_{i}|^{2} + {b_{u}^{user}}^{2} + {b_{i}^{item}}^{2})$  \n",
    "$error = (r_{ui} -\\mu -b_{u}^{user} -b_{i}^{item} -p_{u}q_{i}^{T})$\n",
    "\n",
    "\n",
    "- Compute the partial derivative of given cost function respect to $p_{u}$ and $q_{i}$.  \n",
    "$\\frac{\\partial cost}{\\partial p_{u}} = -error * q_{i} + \\lambda p_{u}$  \n",
    "$\\frac{\\partial cost}{\\partial q_{i}} = -error * p_{u} + \\lambda q_{i}$  \n",
    "$\\frac{\\partial cost}{\\partial b_{u}^{user}} = -error + \\lambda b_{u}^{user}$  \n",
    "$\\frac{\\partial cost}{\\partial b_{i}^{item}} = -error + \\lambda b_{i}^{item}$  \n",
    "\n",
    "\n",
    "- Update the latent of the user and item.  \n",
    "$p_{u} \\leftarrow p_{u} - \\alpha(-error * q_{i} + \\lambda p_{u})$  \n",
    "$q_{i} \\leftarrow q_{i} - \\alpha(-error * p_{u} + \\lambda q_{i})$  \n",
    "$b_{u}^{user} \\leftarrow b_{u}^{user} - \\alpha(-error + \\lambda b_{u}^{user})$  \n",
    "$b_{i}^{item} \\leftarrow b_{i}^{item} - \\alpha(-error + \\lambda b_{i}^{item})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5721f3-2ae2-4505-a8c5-e3972bb2e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3091f473-5df6-4362-8409-4be8b547027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD():\n",
    "\n",
    "    def __init__(self, train, test, k, learning_rate, reg_param, epochs, verbose = False):\n",
    "        \"\"\"\n",
    "        param R : Rating Matrix\n",
    "        param k : latent parameter\n",
    "        param learning_rate : alpha on weight update\n",
    "        param reg_param : regularization parameter\n",
    "        param epochs : training epochs\n",
    "        param verbose : print status\n",
    "        \"\"\"\n",
    "        \n",
    "        self.R = train\n",
    "        self.test = test\n",
    "        self.num_users, self.num_items = train.shape\n",
    "        self.k = k\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_param = reg_param\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        training Matrix Factorization : update matrix latent weight and bias\n",
    "        \"\"\"\n",
    "        # init latent features\n",
    "        self.P = np.random.normal(scale = 1.0/self.k, size = (self.num_users, self.k))\n",
    "        self.Q = np.random.normal(scale = 1.0/self.k, size = (self.num_items, self.k))\n",
    "        \n",
    "        # init biases\n",
    "        self.b_P = np.zeros(self.num_users)\n",
    "        self.b_Q = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "        \n",
    "        self.training_process = []\n",
    "        \n",
    "        start = timer()\n",
    "        \n",
    "        # Start Training!\n",
    "        for epoch in range(self.epochs):\n",
    "            for u in range(self.num_users):\n",
    "                for i in range(self.num_items):\n",
    "                    if self.R[u, i] > 0:\n",
    "                        self.gradient_descent(u, i, self.R[u, i])\n",
    "            \n",
    "            train_cost, test_cost = self.cost()\n",
    "            self.training_process.append((epoch, train_cost, test_cost))\n",
    "            \n",
    "            if self.verbose == True and ((epoch + 1) % 10 == 0 ):\n",
    "                print(\"Iteration : %d, train_cost = %.4f, test_cost = %.4f\" % (epoch+1, train_cost, test_cost))\n",
    "        \n",
    "        print(\"time : %.4f sec\" % (timer() - start) )\n",
    "        \n",
    "    \n",
    "    def cost(self):\n",
    "        \"\"\"\n",
    "        compute RMSE\n",
    "        \"\"\"\n",
    "        xi, yi = self.R.nonzero()\n",
    "        test_x, test_y = self.test.nonzero()\n",
    "        predicted = self.get_complete_matrix()\n",
    "        cost_train = 0\n",
    "        cost_test = 0\n",
    "        \n",
    "        for x, y in zip(xi, yi):\n",
    "            cost_train += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        \n",
    "        for i, j in zip(test_x, test_y):\n",
    "            cost_test += pow(self.test[i, j] - predicted[i, j], 2)\n",
    "        \n",
    "        return np.sqrt(cost_train/len(xi)), np.sqrt(cost_test/len(test_x))\n",
    "        \n",
    "    \n",
    "    def gradient(self, error, u, i):\n",
    "        \"\"\"\n",
    "        gradient of latent feature for GD\n",
    "        param error : rating - prediction error\n",
    "        param u : user index\n",
    "        param i : item index\n",
    "        \"\"\"\n",
    "        dp = (error * self.Q[i, :]) - (self.reg_param * self.P[u, :])\n",
    "        dq = (error * self.P[u, :]) - (self.reg_param * self.Q[i, :])\n",
    "        \n",
    "        return dp, dq\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, u, i, rating):\n",
    "        \"\"\"\n",
    "        gradient descent function\n",
    "        param u : user index\n",
    "        param i : item index\n",
    "        param rating : rating of (u, i)\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = self.get_prediction(u,i)\n",
    "        error = rating - prediction\n",
    "        \n",
    "        self.b_P[u] += self.learning_rate * (error - self.reg_param * self.b_P[u])\n",
    "        self.b_Q[i] += self.learning_rate * (error - self.reg_param * self.b_Q[i])\n",
    "        \n",
    "        dp, dq = self.gradient(error, u, i)\n",
    "        self.P[u, :] += self.learning_rate * dp\n",
    "        self.Q[i, :] += self.learning_rate * dq\n",
    "        \n",
    "    \n",
    "    def get_prediction(self, u, i):\n",
    "        \"\"\"\n",
    "        get predicted rating by user i on item j\n",
    "        \"\"\"\n",
    "        return self.b + self.b_P[u] + self.b_Q[i] + self.P[u, :].dot(self.Q[i, :].T)\n",
    "\n",
    "    \n",
    "    def get_complete_matrix(self):\n",
    "        \"\"\"\n",
    "        computer complete matrix\n",
    "        \"\"\"\n",
    "        return self.b + self.b_P[:, np.newaxis] + self.b_Q[np.newaxis,:] + self.P.dot(self.Q.T)\n",
    "    \n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"\n",
    "        print fit results\n",
    "        \"\"\"\n",
    "        print(\"Final R matrix:\")\n",
    "        print(self.get_complete_matrix())\n",
    "        print(\"\\n\")\n",
    "        print(\"Final RMSE: {:.4f}\".format(self.training_process[self.epochs-1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f29d8e-0757-42be-9d14-6bed502b275b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 10, train_cost = 0.8904, test_cost = 0.9549\n",
      "Iteration : 20, train_cost = 0.6865, test_cost = 0.9361\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-479631021fda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfactorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfactorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mfactorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a4cc72fcbdc0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtrain_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a4cc72fcbdc0>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(self, u, i, rating)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(7)    \n",
    "np.seterr(all=\"warn\")\n",
    "\n",
    "train = data.train\n",
    "test = data.test\n",
    "\n",
    "factorizer = SVD(train, test, k=40, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True)\n",
    "factorizer.fit()\n",
    "factorizer.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720ec59-d1e8-4f77-ac4b-f45cdd12efec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
