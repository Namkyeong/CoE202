{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d7c676-4b78-4d5d-8802-6bad7dc7e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d79fb-64a9-4688-a3cb-7dca2c26f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"/content/drive/My Drive/Colab Notebooks/CoE202/Collaborative Filtering\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcff133-832d-4654-8a37-7fe6877c2d29",
   "metadata": {},
   "source": [
    "## Collaborative Filtering (CF)\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=185fQI_jd3DewJRSKO7TEIZH6Sjr15UGc\" width=\"50%\" height=\"50%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "- The most prominent approach to generate recommendations\n",
    "    - Used by large, commercial e-commerce sites\n",
    "    - Well-understood, various algorithms and variations exist\n",
    "    - Applicable in many domains\n",
    "- Use the **wisdom of the crowd** to recommend items\n",
    "- Basic assumption and idea\n",
    "    - Users give ratings to items (implicitly or explicitly)\n",
    "    - <span style=\"color:red\">**Customers who had similar tastes in the past will have similar tastes in the future**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ce4fa-bc1f-4a22-9546-5217ff88bb2e",
   "metadata": {},
   "source": [
    "### Model-based CF\n",
    "#### Matrix Factorization\n",
    "We can factorize **ratings matrix** into **user matrix** and **item matrix**.\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1880BHOvpFW66QjjjnnN-exW_HOyX9EkU\" width=\"50%\" height=\"50%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "We can model directly leveraging only observed **ratings**, while avoiding overfitting through an adequate regularized model, such as :  \n",
    "$min \\frac{1}{2} \\sum_{(u, i)\\in R}{(r_{ui} -\\mu -b_{u}^{user} -b_{i}^{item} -p_{u}q_{i}^{T})^{2}} + \\lambda(|p_{u}|^{2} + |q_{i}|^{2} + {b_{u}^{user}}^{2} + {b_{i}^{item}}^{2})$  \n",
    "where $p_{u}$ and $q_{i}$ are the latent factor of user $u$ and item $i$, respectively.  \n",
    "$b_{u}$ and $b_{i}$ are the bias term of user $u$ and item $i$, respectively.  \n",
    "$\\mu$ is the mean of the observed ratings matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c71cd-ac06-4e04-85b7-9bf81d5800dd",
   "metadata": {},
   "source": [
    "#### Implicit Feedback\n",
    "Recommender systems rely on many different types of feedbacks.  \n",
    "Netflix collects star ratings for movies and TiVo users indicate tehir preferences for TV shows by hitting thumbs-up/dowm buttons.  \n",
    "However, **explicit feedback is not always available**.\n",
    "Therefore, recommenders can infer user preferences from the more abundant **implicit feedback**,  \n",
    "which indirectly reflect opinion through observing user behavior.\n",
    "\n",
    "Implicit feedback can be collected constantly and do not require additional efforts from user.\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1WXWuwp2dJp6kqUEkQibrvRpiRktl4am7\" width=\"50%\" height=\"50%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "The input data associate users and items through $r_{ui}$ values, which we henceforth call *observations*.  \n",
    "For implicit feedback datasets, those values would indicate observations for user actions.  \n",
    "For example, $r_{ui}$ can indicate the number of items $u$ purchased item $i$ or the time $u$ spent on webpage $i$.\n",
    "\n",
    "Prime characteristics of Implicit Feedback.\n",
    "- No negative feedback\n",
    "- Implicit feedback is inherently noisy\n",
    "- Numerical value of implicit feedback indicates confidence\n",
    "- Evaluation of implicity-feedback recommender requires appropriate measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58d4f3-5b34-496b-99a8-b56c88e967d0",
   "metadata": {},
   "source": [
    "#### OCCF (One Class Collaborative Filtering)\n",
    "\n",
    "[Collaborative Filtering for Implicit Feedback Datasets](http://yifanhu.net/PUB/cf.pdf)\n",
    "\n",
    "OCCF introduces confidence levels on unobserved interaction.\n",
    "- If user $u$ consumed item $i$ ($r_{ui} > 0$), then we have an indication that $u$ likes $i$.\n",
    "- If user $u$ never consumed item $i$, we belive no preference.\n",
    "- i.e., $p_{ui} = \\begin{cases} 1 & r_{ui} > 0 \\\\ 0 & r_{ui} = 0 \\end{cases}$\n",
    "- However, with **varying confidence levels** (Not all consumptions are the same, Not all non-consumptions are the same)\n",
    "- i.e., $c_{ui} = 1 + \\alpha r_{ui}$\n",
    "\n",
    "So we aim to minimize the following equations\n",
    "$min \\sum_{(u, i)}{c_{ui}(p_{ui} -x_{u}^{T}y_{i})^{2}} + \\lambda(\\sum_{u}{||x_{u}||}^{2} + \\sum_{i}{||y_{i}||}^{2})$ \n",
    "\n",
    "From now on, since $p_{ui}$ contains meaning even when $p_{ui}$ equals zero, we should take care of all possible ($u, i$) pairs, rather than only those corresponding to observed data.  \n",
    "Since there are too many pairs, it is not feasible to apply stochastic gradient descent.  \n",
    "Instead, we solve the problem via **Alternating Least Squares (ALS)** algorithm.\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1ihADiG1xa-Xspb661zoBDriWRQI-sdbc\" width=\"80%\" height=\"80%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "OCCF iteratively update the user and item latent with below equations.  \n",
    "$x_{u} = (Y^{T}C^{u}Y + \\lambda I)^{-1}Y^{T}C^{u}p(u)$  \n",
    "$y_{i} = (X^{T}C^{i}X + \\lambda I)^{-1}X^{T}C^{i}p(i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db2ba4-92e2-4e26-9c49-79bf4309b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c352781e-e9a5-43bd-b2ca-16a4fc51f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCCF():\n",
    "    \n",
    "    def __init__(self, train, test, f, epsilon):\n",
    "        \"\"\"\n",
    "        param train : Rating Matrix for train\n",
    "        param test : Rating Matrix for test\n",
    "        param f : latent feature parameter\n",
    "        \"\"\"\n",
    "        \n",
    "        self.R = train # Implication Matrix for training size (m, n)\n",
    "        self.R_test = test # Implication Matrix for test size (m, n)\n",
    "        self.P = np.array(np.vectorize(lambda x: 0 if x==0 else 1)(train), dtype = np.float64) # Preference Matrix for training\n",
    "        self.P_test = np.array(np.vectorize(lambda x: 0 if x==0 else 1)(test), dtype = np.float64) # Preference Matrix for training\n",
    "        self.n_user_rated = np.sum(self.P, axis = 1)\n",
    "        self.n_item_rated = np.sum(self.P, axis = 0)\n",
    "        self.num_users, self.num_items = train.shape\n",
    "        self.alpha = 40\n",
    "        self.reg = 0.002\n",
    "        self.C = 1 + self.alpha * self.R # Confidence Matrix size (m, n)\n",
    "        self.f = f\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        training Matrix Factorization : update matrix latent weight and bias\n",
    "        \"\"\"\n",
    "        # init latent features\n",
    "        self.X = np.random.normal(scale = 1.0/self.f, size=(self.num_users, self.f))\n",
    "        self.Y = np.random.normal(scale = 1.0/self.f, size=(self.num_items, self.f))\n",
    "        \n",
    "        count = 0\n",
    "        cost_diff = 1000000\n",
    "        self.training_process = []\n",
    "        self.cost_list = [0]\n",
    "        time = 0\n",
    "        # repeat ALS until convergence\n",
    "        while cost_diff > self.epsilon :\n",
    "            \n",
    "            start = timer()\n",
    "            count += 1\n",
    "            self.yTy = self.Y.T.dot(self.Y)\n",
    "            for u in range(self.num_users):\n",
    "                self.optimize_x(u)\n",
    "            \n",
    "            self.xTx = self.X.T.dot(self.X)\n",
    "            for i in range(self.num_items):\n",
    "                self.optimize_y(i)\n",
    "            time += (timer() - start)\n",
    "            \n",
    "            cost = self.cost()\n",
    "            self.cost_list.append(cost)\n",
    "            if count > 1 :\n",
    "                cost_diff = self.cost_list[count - 1] - self.cost_list[count]\n",
    "            start_rank = timer()\n",
    "            rank = self.compute_rank()\n",
    "            print(\"time to compute rank : %.4f\" % ( timer() - start_rank ))\n",
    "            self.training_process.append([count, cost_diff, rank])\n",
    "            print(\"count: %d, cost_difference : %.4f, rank : %.4f, time for a epoch : %.4f\"% (count, cost_diff, rank, time))\n",
    "            time = 0\n",
    "\n",
    "                \n",
    "    def optimize_x(self, u):\n",
    "        \"\"\"\n",
    "        Optimize X given user u\n",
    "        \"\"\"\n",
    "        C_u = np.diag(self.C[u, :]) # create diagonal matrix size (n, n)\n",
    "        \n",
    "        # (f,f) matrix\n",
    "        temp1 = self.yTy + self.Y.T.dot(C_u - np.identity(self.num_items)).dot(self.Y) + self.reg * np.identity(self.f)\n",
    "        # (f,1) matrix\n",
    "        temp2 = self.Y.T.dot(C_u).dot(self.P[u])\n",
    "        \n",
    "        self.X[u, :] = np.linalg.solve(temp1, temp2)\n",
    "        \n",
    "    \n",
    "    def optimize_y(self, i):\n",
    "        \"\"\"\n",
    "        Optimize X given user u\n",
    "        \"\"\"\n",
    "        C_i = np.diag(self.C[:, i]) # create diagonal matrix size (m, m)\n",
    "        \n",
    "        # (f,f) matrix\n",
    "        temp1 = self.xTx + self.X.T.dot(C_i - np.identity(self.num_users)).dot(self.X) + self.reg * np.identity(self.f)\n",
    "        # (f,1) matrix\n",
    "        temp2 = self.X.T.dot(C_i).dot(self.P[:, i])\n",
    "        \n",
    "        self.Y[i, :] = np.linalg.solve(temp1, temp2)\n",
    "        \n",
    "  \n",
    "    def cost(self):\n",
    "        \"\"\"\n",
    "        compute Loss function\n",
    "        \"\"\"\n",
    "        loss = np.sum(self.C * np.square(self.P - self.X.dot(self.Y.T))) + self.reg * (np.linalg.norm(self.X) + np.linalg.norm(self.Y))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def compute_rank(self):\n",
    "        \n",
    "        prediction = self.X.dot(self.Y.T)\n",
    "        temp_1 = 0\n",
    "        temp_2 = 0\n",
    "        \n",
    "        for x in range(self.num_users) :\n",
    "            inv_pre = -1 * prediction[x, :]\n",
    "            sort_x = inv_pre.argsort() # index starts with 0\n",
    "            sort_x = sort_x.argsort()\n",
    "            rank_x = sort_x / len(sort_x)\n",
    "            \n",
    "            temp_1 += (self.R_test[x, :] * rank_x).sum()\n",
    "            temp_2 += self.R_test[x, :].sum()\n",
    "        \n",
    "        rank = temp_1 / temp_2\n",
    "            \n",
    "        return rank\n",
    "    \n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"\n",
    "        print fit results\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Final P hat matrix:\")\n",
    "        print(self.X.dot(self.Y.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97298ae1-f99b-4aab-84d7-cf80b8990e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)    \n",
    "np.seterr(all=\"warn\")\n",
    "\n",
    "train = data.train\n",
    "test = data.test\n",
    "\n",
    "factorizer = MatrixFactorization(train, test, f=40, epsilon = 100)\n",
    "factorizer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fab2e8-457e-4630-8086-5c6f6ee4eede",
   "metadata": {},
   "source": [
    "#### BPR : Bayesian Personalized Ranking from Implicit Feedback\n",
    "[BPR : Bayesian Personalized Ranking from Implicit Feedback](https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf)  \n",
    "\n",
    "Basically, collaborative filtering models we learned in class are designed for the item prediction task of **personalized ranking**.  \n",
    "But none of them is directly optimized for ranking.  \n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1zoqtL1RQZaHLDLn9aCR4nuZ2ITxtHHbg\" width=\"70%\" height=\"70%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b5071-1b1a-46be-90bf-296d916e028a",
   "metadata": {},
   "source": [
    "BPR presents a generic optimization criterion *BPR-OPT* for personalized ranking that is maximum posterior estimator derived from a Bayesian analysis of the problem.  \n",
    "\n",
    "$p(\\theta|>_{u}) \\propto p(>_{u}|\\theta)p(\\theta)$  \n",
    "\n",
    "$\\prod_{u \\in U}{p(\\theta|>_{u})} = \\prod_{(u, i, j) \\in U \\times I \\times I}{p(i >_{u} j|\\theta)^{\\delta((u, i, j)\\in D_{s})}{(1 - p(i >_{u} j|\\theta))^{\\delta((u, j, i)\\notin D_{s})}}}$  \n",
    "\n",
    "where $\\delta$ is the indicator function:  \n",
    "\n",
    "$\\delta(b) := \\begin{cases} 1 & \\text{if}~b~\\text{is true} \\\\ 0 & \\text{else} \\end{cases}$\n",
    "\n",
    "The above formula can be simplified to \n",
    "\n",
    "$\\prod_{u \\in U}{p(>_{u}|\\theta)} = \\prod_{(u, i, j) \\in D_{s}}{p(i >_{u} j|\\theta)}$\n",
    "\n",
    "We define the individual probability that a user really prefers item *i* to items *j* as:\n",
    "\n",
    "${p(i >_{u} j|\\theta)} = \\sigma(\\hat{x}_{uij})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f12a83-3e03-4c5d-83f5-641c49521ea1",
   "metadata": {},
   "source": [
    "Finally we formulate the maximum posterior estimator to derive our generic optimization criterion for personalized ranking BPR-OPT:  \n",
    "\n",
    "**BPR ORT**  \n",
    "$ = \\ln p(\\theta | >_{u})$  \n",
    "\n",
    "$ = \\ln p(>_{u}| \\theta)p(\\theta)$  \n",
    "\n",
    "$ = \\prod_{(u, i, j) \\in D_{s}}{\\sigma(\\hat{x}_{uij})p(\\theta)}$  \n",
    "\n",
    "$ = \\sum_{(u, i, j) \\in D_{s}}{\\ln\\sigma(\\hat{x}_{uij}) + \\ln p(\\theta)}$  \n",
    "\n",
    "$ = \\sum_{(u, i, j) \\in D_{s}}{\\ln\\sigma(\\hat{x}_{uij}) - \\lambda_{\\theta} {\\|\\theta\\|}^{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c410e671-679b-473e-a173-f07d83c90966",
   "metadata": {},
   "source": [
    "#### BPR Learning Algorithm\n",
    "\n",
    "We optimise the criteria using **bootstrapping** based **stochastic gradient descent**.\n",
    "\n",
    "As SVD last lecture, We will use stochastic gradient descent to update the paramters.\n",
    "\n",
    "$\\frac{\\partial BPR-OPT}{\\partial \\theta} = \\sum_{(u,i,j) \\in D_{S}}{\\frac{e^{-\\hat{x}_{uij}}}{1 + e^{-\\hat{x}_{uij}}}\\cdot\\frac{\\partial}{\\partial \\theta}\\hat{x}_{uij}} - \\lambda_{\\theta}{\\theta} $\n",
    "\n",
    "Please note that we are going to maximize the posterior, so we update the parameters to **ascent direction**.\n",
    "\n",
    "$\\theta \\leftarrow \\theta +\\alpha \\frac{\\partial BPR-OPT}{\\partial \\theta}$\n",
    "\n",
    "Because in our optimization we have triples $(u, i, j) \\in D_S$, we first decompose the estimator $\\hat{x}_{uij}$ and define it as:  \n",
    "$\\hat{x}_{uij} = \\hat{x}_{ui} - \\hat{x}_{uj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e57aa-69e0-47bd-90b5-d95d531c0235",
   "metadata": {},
   "source": [
    "#### Applying BPR with Matrix Factorization\n",
    "Remember that the problem of predicting $\\hat{x}_{ui}$ can be seen as the task of estimating a matrix $X : U \\times I$  \n",
    "Target matrix $X$ is approximated by the matrix product of two low-rank matrices $W : |U| \\times k$ and $H : |I| \\times k$ :   \n",
    "$\\hat{X} = WH^{T}$  \n",
    "$\\hat{x}_{ui} = <w_u, h_i> = \\sum_{f = 1}^{k}{w_{uf} \\cdot h_{if}}$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta}{\\hat{x}_{uij}} = \\begin{cases} h_{if}-h_{jf} & \\text{if}~\\theta = w_{uf} \\\\ w_{uf} & \\text{if}~\\theta = h_{if} \\\\ -w_{uf} & \\text{if}~\\theta = h_{jf} \\\\ 0 & \\text{else} \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed058e3-9461-4b79-b276-ed925053deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import data\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60126758-4f12-426d-b6cf-8dfcd28706a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR():\n",
    "    \n",
    "    def __init__(self, data, train, test, k, learning_rate, reg_param, epsilon):\n",
    "        \"\"\"\n",
    "        param R : Rating Matrix\n",
    "        param k : latent parameter\n",
    "        param learning_rate : alpha on weight update\n",
    "        param reg_param : regularization parameter\n",
    "        \"\"\"\n",
    "        self.A_I = np.array(np.vectorize(lambda x: 0 if x==0 else 1)(data), dtype = np.float64)\n",
    "        self.X = np.array(np.vectorize(lambda x: 0 if x==0 else 1)(train), dtype = np.float64) # create X matrix : implicit feedbacks (binary)\n",
    "        self.X_test = np.array(np.vectorize(lambda x: 0 if x==0 else 1)(test), dtype = np.float64)\n",
    "        self.num_users, self.num_items = train.shape\n",
    "        self.k = k\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_param = reg_param\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        training Matrix Factorization : update matrix latent weight and bias\n",
    "        \"\"\"\n",
    "        \n",
    "        # init latent features\n",
    "        self.W = np.random.normal(scale = 1.0/self.k, size=(self.num_users, self.k))\n",
    "        self.H = np.random.normal(scale = 1.0/self.k, size=(self.num_items, self.k))\n",
    "        \n",
    "        # train until cost converges\n",
    "        count = 0\n",
    "        self.training_process = []\n",
    "        for j in range(10):\n",
    "            start = timer()\n",
    "            for i in range(80000) :    \n",
    "\n",
    "                count += 1\n",
    "                # randomly choice_Bootstrap\n",
    "                u = random.choice(self.X.nonzero()[0])\n",
    "                i = random.choice(self.X[u].nonzero()[0]) \n",
    "                j = random.choice(np.argwhere(self.X[u] == 0).T[0]) \n",
    "                self.gradient_descent(u, i, j)\n",
    "            print(\"complete 80000 iterations, time :%.4f\" % (timer()-start))\n",
    "\n",
    "            start_AUC = timer()\n",
    "            AUC = self.compute_AUC()\n",
    "            self.training_process.append((count, AUC))\n",
    "            print(\"Iteration : %d, AUC = %.4f, AUC computation time: %.4f\" % (count, AUC, timer()-start_AUC))\n",
    "\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        return sigmoid \n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    \n",
    "    def gradient_descent(self, u, i, j):\n",
    "        \"\"\"\n",
    "        gradient descent function\n",
    "        param u : user index\n",
    "        param i : item index i\n",
    "        param j : item index j\n",
    "        \"\"\"\n",
    "        xuij_hat = self.W[u].dot(self.H[i].T) - self.W[u].dot(self.H[j].T)\n",
    "        sigmoid = self.sigmoid(xuij_hat) * np.exp(-xuij_hat)\n",
    "        self.W[u, :] += self.learning_rate * (sigmoid * (self.H[i] - self.H[j]) - self.reg_param * self.W[u])\n",
    "        self.H[i, :] += self.learning_rate * (sigmoid * self.W[u] - self.reg_param * self.H[i])\n",
    "        self.H[j, :] += self.learning_rate * (-1 * sigmoid * self.W[u] - self.reg_param * self.H[j])\n",
    "        \n",
    "        \n",
    "    def compute_AUC(self):\n",
    "        \n",
    "        self.X_hat = self.W.dot(self.H.T)\n",
    "        u_nonzero, i_nonzero = self.X_test.nonzero()\n",
    "        num = 0\n",
    "        \n",
    "        for u in u_nonzero :\n",
    "            temp = 0\n",
    "            temp_i = self.X_test[u].nonzero()[0]\n",
    "            temp_j = np.argwhere(self.A_I[u] == 0).T[0]\n",
    "            for i in temp_i :\n",
    "                for j in temp_j :\n",
    "                    if self.X_hat[u, i] > self.X_hat[u, j] :\n",
    "                        temp += 1\n",
    "            num += (temp / (len(temp_i)*len(temp_j)))\n",
    "        auc = num / len(u_nonzero)\n",
    "        \n",
    "        return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a178aed-2bf0-40f3-8e7d-b89ac54c2dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "np.seterr(all=\"warn\")\n",
    "\n",
    "ml_100k = data.ml_100k\n",
    "train = data.train\n",
    "test = data.test\n",
    "\n",
    "factorizer = BPR(ml_100k, train, test, k=40, learning_rate=0.1, reg_param=0.01, epsilon = 0.1)\n",
    "\n",
    "factorizer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d73a99-2a40-4dc2-9bc8-129d74a3c139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
