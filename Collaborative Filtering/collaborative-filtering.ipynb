{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d7c676-4b78-4d5d-8802-6bad7dc7e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d79fb-64a9-4688-a3cb-7dca2c26f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"/content/drive/My Drive/Colab Notebooks/CoE202/Collaborative Filtering\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcff133-832d-4654-8a37-7fe6877c2d29",
   "metadata": {},
   "source": [
    "## Collaborative Filtering (CF)\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=185fQI_jd3DewJRSKO7TEIZH6Sjr15UGc\" width=\"50%\" height=\"50%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "- The most prominent approach to generate recommendations\n",
    "    - Used by large, commercial e-commerce sites\n",
    "    - Well-understood, various algorithms and variations exist\n",
    "    - Applicable in many domains\n",
    "- Use the **wisdom of the crowd** to recommend items\n",
    "- Basic assumption and idea\n",
    "    - Users give ratings to items (implicitly or explicitly)\n",
    "    - <span style=\"color:red\">**Customers who had similar tastes in the past will have similar tastes in the future**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2befc68a-000e-4180-883a-769802c4012f",
   "metadata": {},
   "source": [
    "### Neighborhood-based CF\n",
    "- Main idea\n",
    "    - Similar users display similar patterns of rating behavior (<span style=\"color:red\">**User-based CF**</span>)\n",
    "    - Similar items receive similar ratings (<span style=\"color:red\">**Item-based CF**</span>)\n",
    "- How do we define similarity between users and items?\n",
    "    - We define similarity between users in terms of items they purchased!\n",
    "    - We define similarity between items in terms of users who purchased them!\n",
    "    - We learned variety of similarity measures in the lecture (e.g., Euclidean distance, Jaccard similarity, Cosine similarity, Pearson correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ce4fa-bc1f-4a22-9546-5217ff88bb2e",
   "metadata": {},
   "source": [
    "### Model-based CF\n",
    "#### Matrix Factorization\n",
    "As we learned in the lecture, we can factorize **ratings matrix** into **user matrix** and **item matrix**.\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1880BHOvpFW66QjjjnnN-exW_HOyX9EkU\" width=\"50%\" height=\"50%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "Ratings can be interpreted as **dot product** of user latent and item latent.  \n",
    "So we can obtain user/item matrix by decomposing ratings matrix,  \n",
    "and predict unobserved ratings with obtained user/item matrix.\n",
    "\n",
    "Okay then, how can we decompose the ratings matrix into user and item latent matrix?\n",
    "\n",
    "**Singular Value Decomposition (SVD)** is a famous linear algebra technique for matrix factorization.  \n",
    "And it is often used for **dimensionality reduction**.  \n",
    "\n",
    "If you are not familiar with SVD, please refer to basic linear algebra class or chapter 4 in below linked book. (https://mml-book.github.io/book/mml-book.pdf)\n",
    "\n",
    "\n",
    "However, applying SVD to recommender system have the following issues.\n",
    "- Predicted values are often negative.\n",
    "- Zero replacement decreases prediction quality.\n",
    "    - <span style=\"color:red\">The meaning of \"zero\" is different from that of \"unknown\"</span>.\n",
    "    - We should be careful whether we want to set missing values to zero.\n",
    "\n",
    "\n",
    "So, we can model directly leveraging **only observed ratings**, while **avoiding overfitting** through an adequate regularized model, such as :  \n",
    "$min \\frac{1}{2} \\sum_{(u, i)\\in R}{(r_{ui} -\\mu -b_{u}^{user} -b_{i}^{item} -p_{u}q_{i}^{T})^{2}} + \\lambda(|p_{u}|^{2} + |q_{i}|^{2} + {b_{u}^{user}}^{2} + {b_{i}^{item}}^{2})$  \n",
    "where $p_{u}$ and $q_{i}$ are the latent factor of user $u$ and item $i$, respectively.  \n",
    "$b_{u}$ and $b_{i}$ are the bias term of user $u$ and item $i$, respectively.  \n",
    "$\\mu$ is the mean of the observed ratings matrix.  \n",
    "\n",
    "\n",
    "A simple **gradient descent** technique can be applied to solve the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000571ba-ff00-43f2-a05e-3f9cdc1a2162",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "In optimization problem, we have two approaches to find solution of the problem.  \n",
    "First, we can simply solve the problem in **closed form**.  \n",
    "However, in the case of complex function, it may not be feasible to solve in the closed form.  \n",
    "To address the issue, we can iteratively search the solution space improving the target value. We call such a method as **improving search**.\n",
    "\n",
    "Gradient descent is a first-order iterative optimization algorithm for finding local minimum of a differentiable function.  \n",
    "The idea is to take repeated steps in the **opposite direction of the gradient** of the function at the current point, because this is the direction of steepest descent.\n",
    "\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=18o3tBBj9roZsHkLXdatayiGc3faF0zrM\" width=\"50%\" height=\"50%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "If gradient > 0 , then increasing the weight will increase the cost function.  \n",
    "If gradient < 0 , then increasing the weight will decrease the cost function.\n",
    "\n",
    "So we update the weight (parameters) with the partial derivative of the cost.  \n",
    "$w \\leftarrow w - \\alpha \\frac{\\partial Cost}{\\partial w}$\n",
    "\n",
    "\n",
    "\n",
    "Gradient descent is very general algorithm which is very **effective** and **scalable**.  \n",
    "However, we should use gradient descent taking the below in regard.\n",
    "- **Local minima**\n",
    "    - Sensitive to the starting point\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=18loEZYzhQAZj0A6192fwIGZ0M2rHeXUO\" width=\"40%\" height=\"40%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "- **Learning rate**\n",
    "    - Too large? Too small?\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=18ZvvZVl3oTAo9mzILltbE-S-3Cq_a3Yt\" width=\"40%\" height=\"40%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d151f79-1e82-463e-96c3-da1abd55970d",
   "metadata": {},
   "source": [
    "#### Matrix Factorization\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=186dDOMvTxDXGrVqW1kke74zMQT3DvE9e\" width=\"70%\" height=\"70%\" title=\"recommender system\" alt=\"recommender system\"></img>\n",
    "</figure>\n",
    "\n",
    "We can minimize the below cost function with gradient descent.  \n",
    "$min \\frac{1}{2} \\sum_{(u, i)\\in R}{(r_{ui} -\\mu -b_{u}^{user} -b_{i}^{item} -p_{u}q_{i}^{T})^{2}} + \\lambda(|p_{u}|^{2} + |q_{i}|^{2} + {b_{u}^{user}}^{2} + {b_{i}^{item}}^{2})$  \n",
    "$error = (r_{ui} -\\mu -b_{u}^{user} -b_{i}^{item} -p_{u}q_{i}^{T})$\n",
    "\n",
    "\n",
    "- Compute the partial derivative of given cost function respect to $p_{u}$ and $q_{i}$.  \n",
    "$\\frac{\\partial cost}{\\partial p_{u}} = -error * q_{i} + \\lambda p_{u}$  \n",
    "$\\frac{\\partial cost}{\\partial q_{i}} = -error * p_{u} + \\lambda q_{i}$  \n",
    "$\\frac{\\partial cost}{\\partial b_{u}^{user}} = -error + \\lambda b_{u}^{user}$  \n",
    "$\\frac{\\partial cost}{\\partial b_{i}^{item}} = -error + \\lambda b_{i}^{item}$  \n",
    "\n",
    "\n",
    "- Update the latent of the user and item.  \n",
    "$p_{u} \\leftarrow p_{u} - \\alpha(-error * q_{i} + \\lambda p_{u})$  \n",
    "$q_{i} \\leftarrow q_{i} - \\alpha(-error * p_{u} + \\lambda q_{i})$  \n",
    "$b_{u}^{user} \\leftarrow b_{u}^{user} - \\alpha(-error + \\lambda b_{u}^{user})$  \n",
    "$b_{i}^{item} \\leftarrow b_{i}^{item} - \\alpha(-error + \\lambda b_{i}^{item})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5721f3-2ae2-4505-a8c5-e3972bb2e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3091f473-5df6-4362-8409-4be8b547027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD():\n",
    "\n",
    "    def __init__(self, train, test, k, learning_rate, reg_param, epochs, verbose = False):\n",
    "        \"\"\"\n",
    "        param R : Rating Matrix\n",
    "        param k : latent parameter\n",
    "        param learning_rate : alpha on weight update\n",
    "        param reg_param : regularization parameter\n",
    "        param epochs : training epochs\n",
    "        param verbose : print status\n",
    "        \"\"\"\n",
    "        \n",
    "        self.R = train\n",
    "        self.test = test\n",
    "        self.num_users, self.num_items = train.shape\n",
    "        self.k = k\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_param = reg_param\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        training Matrix Factorization : update matrix latent weight and bias\n",
    "        \"\"\"\n",
    "        # init latent features\n",
    "        self.P = np.random.normal(scale = 1.0/self.k, size = (self.num_users, self.k))\n",
    "        self.Q = np.random.normal(scale = 1.0/self.k, size = (self.num_items, self.k))\n",
    "        \n",
    "        # init biases\n",
    "        self.b_P = np.zeros(self.num_users)\n",
    "        self.b_Q = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "        \n",
    "        self.training_process = []\n",
    "        \n",
    "        start = timer()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for u in range(self.num_users):\n",
    "                for i in range(self.num_items):\n",
    "                    if self.R[u, i] > 0:\n",
    "                        self.gradient_descent(u, i, self.R[u, i])\n",
    "            \n",
    "            train_cost, test_cost = self.cost()\n",
    "            self.training_process.append((epoch, train_cost, test_cost))\n",
    "            \n",
    "            if self.verbose == True and ((epoch + 1) % 10 == 0 ):\n",
    "                print(\"Iteration : %d, train_cost = %.4f, test_cost = %.4f\" % (epoch+1, train_cost, test_cost))\n",
    "        \n",
    "        print(\"time : %.4f sec\" % (timer() - start) )\n",
    "        \n",
    "    \n",
    "    def cost(self):\n",
    "        \"\"\"\n",
    "        compute RMSE\n",
    "        \"\"\"\n",
    "        xi, yi = self.R.nonzero()\n",
    "        test_x, test_y = self.test.nonzero()\n",
    "        predicted = self.get_complete_matrix()\n",
    "        cost_train = 0\n",
    "        cost_test = 0\n",
    "        \n",
    "        for x, y in zip(xi, yi):\n",
    "            cost_train += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        \n",
    "        for i, j in zip(test_x, test_y):\n",
    "            cost_test += pow(self.test[i, j] - predicted[i, j], 2)\n",
    "        \n",
    "        return np.sqrt(cost_train/len(xi)), np.sqrt(cost_test/len(test_x))\n",
    "        \n",
    "    \n",
    "    def gradient(self, error, u, i):\n",
    "        \"\"\"\n",
    "        gradient of latent feature for GD\n",
    "        param error : rating - prediction error\n",
    "        param u : user index\n",
    "        param i : item index\n",
    "        \"\"\"\n",
    "        dp = (error * self.Q[i, :]) - (self.reg_param * self.P[u, :])\n",
    "        dq = (error * self.P[u, :]) - (self.reg_param * self.Q[i, :])\n",
    "        \n",
    "        return dp, dq\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, u, i, rating):\n",
    "        \"\"\"\n",
    "        gradient descent function\n",
    "        param u : user index\n",
    "        param i : item index\n",
    "        param rating : rating of (u, i)\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = self.get_prediction(u,i)\n",
    "        error = rating - prediction\n",
    "        \n",
    "        self.b_P[u] += self.learning_rate * (error - self.reg_param * self.b_P[u])\n",
    "        self.b_Q[i] += self.learning_rate * (error - self.reg_param * self.b_Q[i])\n",
    "        \n",
    "        dp, dq = self.gradient(error, u, i)\n",
    "        self.P[u, :] += self.learning_rate * dp\n",
    "        self.Q[i, :] += self.learning_rate * dq\n",
    "        \n",
    "    \n",
    "    def get_prediction(self, u, i):\n",
    "        \"\"\"\n",
    "        get predicted rating by user i on item j\n",
    "        \"\"\"\n",
    "        return self.b + self.b_P[u] + self.b_Q[i] + self.P[u, :].dot(self.Q[i, :].T)\n",
    "\n",
    "    \n",
    "    def get_complete_matrix(self):\n",
    "        \"\"\"\n",
    "        computer complete matrix\n",
    "        \"\"\"\n",
    "        return self.b + self.b_P[:, np.newaxis] + self.b_Q[np.newaxis,:] + self.P.dot(self.Q.T)\n",
    "    \n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"\n",
    "        print fit results\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Final R matrix:\")\n",
    "        print(self.get_complete_matrix())\n",
    "        print(\"\\n\")\n",
    "        print(\"Final RMSE: {:.4f}\".format(self.training_process[self.epochs-1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f29d8e-0757-42be-9d14-6bed502b275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.train\n",
    "test = data.test\n",
    "\n",
    "factorizer = SVD(train, test, k=40, learning_rate=0.01, reg_param=0.01, epochs=30, verbose=True)\n",
    "factorizer.fit()\n",
    "factorizer.print_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
